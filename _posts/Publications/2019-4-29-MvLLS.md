---
layout: post
title:  "Multi-view laplacian least squares for human emotion recognition "
subtitle: "Shuai Guo, Lin Feng, Zhan-Bo Feng et. al. Neurocomputing, Submitted on 2019.05 "
date:   2019-4-29
comments: true
categories: Publications
---

* content
{:toc}

Under review now. <a href="/downloads/MvLLS.pdf" target="_blank">[Full text]<a>. 

### My works
* Put forward the idea
* Conduct experiments
* Write the paper

## 1. Overview
* In this paper, a flexible and extensible nonlinear method multi-view laplacian least square (MvLLS) is proposed for multi-view human-emotion recognition. 
* MvLLS finds a common subspace across all views where the connected samples stay close to each other as well. 
* With the global laplacian weighted graph (GLWP), MvLLS records the local structures, introduces the category discriminant information, and protects the local information. 
* MvLLS is optimized with interactive method. 
* The weighted local preserving embedding (WLPE) is the out-of-sample extension of MvLLS. 

<div align="center"><img src="/images/MvLLS.png"></div> 

## 2. Motivations
* There are few nonlinear subspace learning methods proposed. 
* The lack of category information or features of some samples could make most of existing methods hard to work. 
* PLS get much better experimental results compared with CCA. 

## 3. Multi-view laplacian least squares

### 3.1. Global laplacian weighted graph
A global laplacian weighted graph (GLWP) $$\mathcal{W}$$ is constructed over all views, which is composed by weighted graph of each two views $$W$$. Whether two samples in $$W$$ are connected or not depends on labels of they and their neighbors. In this way, the category discriminant information and locality is protected. 

### 3.2. Laplacian partial least squares
Inspired by PLS, after getting $$\mathcal{Y}$$ which is the embeddings of all views on the first dimension of the subspace with $$\mathcal{W}$$, 
we use $$\mathcal{Y}$$ as the regressor of $$\mathcal{W}$$ to predict it. Then $$\mathcal{W}$$ is replaced by the residual matrix. This procedure is repeated for $dim$ times to get the embeddings on each dimension of the target subspace. 

### 3.3. Weighted local preserving embedding
WLPE looks for an embedding in the subspace that maintains the local neighbors of the high-dimensional space, and the neighbors with higher degrees in GLWM are expected to have greater weights. The embeddings of new samples are regarded as weighted sum of embeddings of its neighbors. 

## 4. Evaluation
We evaluate MvLLS with some state-of-art methods on two datasets and three tasks, experiment results indicate the effectiveness of the proposed method. 